\documentclass{article}


\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Deep Learning Project: GAN Artwork}


\author{
	Kevin Steele \\
	Department of Electrical Engineering \\
	University of Iowa\\
	\texttt{kevin-steele@uiowa.edu} \\
	%% examples of more authors
	\And
	Brain Fiegel \\
	Department of Industrial and Mechanical Engineering \\
	University of Iowa\\
	\texttt{brian-fiegel@uiowa.edu} \\
	\And
	Stjepan Fiolic \\
	Department of Electrical Engineering \\
	University of Iowa\\
	\texttt{stjepan-fiolic@uiowa.edu} \\
}

\begin{document}
	\maketitle
	
	\begin{abstract}
		https://www.overleaf.com/latex/templates/style-and-template-for-preprints-arxiv-bio-arxiv/fxsnsrzpnvwc
		The Iowa Neuroscience Institute has a desire for a landscape generation process by which DNA sequences are converted to landscape images. Our project is to train a Generative Adversarial Network (GAN) to accomplish this task.
	\end{abstract}
	
	
	% keywords can be removed
	\keywords{Deep Learning \and GAN}
	
	
	\section{Introduction}

	
	This project will create artworks using the DNA sequence of a person as a seed. A generative adversarial network (GAN) will be trained on images of landscape paintings and be tuned to produce a customized artwork for each different DNA sequence. If successful, the algorithm will be deployed at the Iowa Neuroscience Institute to create souvenirs for research subjects.
	
	Testing
	
	\section{Related Works (Optional)}
	\label{sec:headings}
	
	\begin{itemize}
		\item If you find it necessary to have a separate section to deep dive into other relevant works in literature, create a section titled "Related Work".
		\item Try to categorize your/other people's previous works and build a *story*. Don't just list some facts. The worst thing to do is something like: "Smith et al. did this. Johnson et al. did that. Xu et al. did this. ..." ----a simple listing of papers. Instead, tell a story (like a fun history book).
		\item For the scope of this project, it is not always necessary.
		\item If you plan to transform this course project into a journal/conference submission, it should be helpful to have it written up.
	\end{itemize}
	
	See Section \ref{sec:headings}.
	
	
	\section{Problem Definition}
	\label{sec:others}
	Our goal is to convert a valid DNA input sequence into a landscape image using a Generative Adversarial Network (GAN). This is a Text-to-Image synthesis issue, with the generator taking valid DNA codons (TCAG) as input and creating landscape 224 by 224 by 3 (RBG) images as output. Image values range within [0,255] across the 3 color channels. Similar to a more general Text-to-Image GAN, which would take text descriptions to generate images, the DNA codon sequence will be a translated language by which we generate landscapes. Valid DNA codons follow the standard generic code table with valid inputs of T, C, A, and G in groups of 3. Each sequence must also start with a start codon and end with an end codon. Noted in the original table, there are 25 different amino acids that these codons correspond to. While it is not currently our goal, this fact could be used similarly to an alphabet, and thusly a sentence or description of the image. 
	
	Looking more comprehensively at the GAN, the generator takes as input valid codon, considered the latent space of the system, and outputs the image we desire. The discriminator would take as input an image, either directly from the dataset or from the generator, and return the probability that the image is real (direct input) or fake (generated input). It is obvious that, generally, DNA does not map to images in the real world. In this case, we have a set of unmapped inputs to model, which will require a specialized GAN architecture. A few architectures have been proposed in this realm, particularly StackGAN \cite{hadash2018estimate} for text-to-image and CycleGAN \cite{kour2014real,kour2014fast} for unpaired image mapping (though the principle works for text \cite{hadash2018estimate}). This involves mapping encoded text to image feature maps generated by the generator, then do the reverse with an image encoder. A more recent attempt at this is MirrorGAN \cite{hadash2018estimate}.  
	
	For our purposes, we currently create input to the generator, the latent variables, by converting the DNA sequence to a normalized average value between [0, 1]. Thus, we convert the variable length string into a single floating-point feature. We also have plans to test using an Auto-Encoder to create this latent variable, as it works as a trainable solution. 
	
	
	The documentation for \verb+natbib+ may be found at
	\begin{center}
		\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
	\end{center}
	Of note is the command \verb+\citet+, which produces citations
	appropriate for use in inline text.  For example,
	\begin{verbatim}
	\citet{hasselmo} investigated\dots
	\end{verbatim}
	produces
	\begin{quote}
		Hasselmo, et al.\ (1995) investigated\dots
	\end{quote}
	
	\begin{center}
		\url{https://www.ctan.org/pkg/booktabs}
	\end{center}
	
	
	\subsection{Figures}
	See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}

	
	\begin{figure}
		\centering
		\fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
		\caption{Sample figure caption.}
		\label{fig:fig1}
	\end{figure}
	
	\subsection{Tables}
	See awesome Table~\ref{tab:table}.
	
	\begin{table}
		\caption{Sample table title}
		\centering
		\begin{tabular}{lll}
			\toprule
			\multicolumn{2}{c}{Part}                   \\
			\cmidrule(r){1-2}
			Name     & Description     & Size ($\mu$m) \\
			\midrule
			Dendrite & Input terminal  & $\sim$100     \\
			Axon     & Output terminal & $\sim$10      \\
			Soma     & Cell body       & up to $10^6$  \\
			\bottomrule
		\end{tabular}
		\label{tab:table}
	\end{table}
	
	\subsection{Lists}
	\begin{itemize}
		\item Lorem ipsum dolor sit amet
		\item consectetur adipiscing elit. 
		\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
	\end{itemize}
	
	
	\section{Data}
	
	
	\bibliographystyle{unsrt}  
	%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
	%%% and comment out the ``thebibliography'' section.
	
	
	%%% Comment out this section when you \bibliography{references} is enabled.
	\begin{thebibliography}{1}
		
		\bibitem{kour2014real}
		George Kour and Raid Saabne.
		\newblock Real-time segmentation of on-line handwritten arabic script.
		\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
			International Conference on}, pages 417--422. IEEE, 2014.
		
		\bibitem{kour2014fast}
		George Kour and Raid Saabne.
		\newblock Fast classification of handwritten on-line arabic characters.
		\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
			International Conference of}, pages 312--318. IEEE, 2014.
		
		\bibitem{hadash2018estimate}
		Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
		Jacovi.
		\newblock Estimate and replace: A novel approach to integrating deep neural
		networks with existing applications.
		\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.
		
	\end{thebibliography}
	
	
\end{document}