{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISE 6380: Chernobyl Blue Chillers - GANdinsky.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHE8cdQMTO1U",
        "outputId": "191f6177-f571-4ff7-f9a4-e93c0fa5eaca"
      },
      "source": [
        "!pip install biopython\n",
        "\n",
        "import Bio\n",
        "from Bio import SeqIO\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import os\n",
        "import pathlib\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "import urllib.request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def prep_drive(doMount, dirsToMake):\n",
        "  #mount google drive for longterm storage\n",
        "  if True == doMount:\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "  for d in dirsToMake:\n",
        "    if not os.path.isdir(d):\n",
        "      os.makedirs(d)\n",
        "\n",
        "def download_and_extract_file(url, local_bundle_path, extraction_directory='.'):\n",
        "  initial_directory = os.getcwd()\n",
        "  os.chdir(extraction_directory)\n",
        "  try:\n",
        "    if not os.path.exists(local_bundle_path):\n",
        "      # download the dataset\n",
        "      urllib.request.urlretrieve(url,local_bundle_path )\n",
        "      \n",
        "      if local_bundle_path.endswith('.zip'):\n",
        "        # extract the dataset and store it on google drive\n",
        "        with ZipFile(local_bundle_path, 'r') as zip:\n",
        "          zip.extractall()\n",
        "      elif local_bundle_path.endswith('.tar.gz') or local_bundle_path.endswith('.tgz'):\n",
        "        tar = tarfile.open(local_bundle_path, \"r:gz\")\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "      elif local_bundle_path.endswith('.tar'):\n",
        "        tar = tarfile.open(local_bundle_path, \"r:\")\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "      elif path.endswith('.tar.bz2') or path.endswith('.tbz'):\n",
        "        tar = tarfile.open(local_bundle_path, \"r:bz2\")\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "      else: \n",
        "        raise Exception(local_bundle_path + \" has an unrecognized file extension\")\n",
        "  finally:\n",
        "    os.chdir(initial_directory)\n",
        "\n",
        "def noop():\n",
        "  return None\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (1.78)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYGpfA8tv6f"
      },
      "source": [
        "# Starting to build our GAN, based on the sample provided by TensorFlow\n",
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
        "\n",
        "class GANDinsky(GAN):\n",
        "    def __init__(self, useSampleData):\n",
        "      latent_dim = 128\n",
        "      discriminator =tf.keras.Sequential(\n",
        "            [\n",
        "             \n",
        "            ],\n",
        "            name=\"discriminator\",\n",
        "        )\n",
        "      generator =tf.keras.Sequential(\n",
        "              [\n",
        "              ],\n",
        "              name=\"generator\",\n",
        "          )\n",
        "      # finish construction of a basic GAN\n",
        "      super(GANDinsky, self).__init__(\n",
        "          discriminator=discriminator,\n",
        "          generator=generator,\n",
        "          latent_dim=128)\n",
        "      \n",
        "      # Prep GANdisky specific data members\n",
        "      self.TrainingParam={\n",
        "          \"batch_size\":0,\n",
        "          \"height\":256,\n",
        "          \"width\":256,\n",
        "          \"validation_split\":0.2,\n",
        "          \"seed\":5549\n",
        "        }\n",
        "\n",
        "      self.UseSampleData = useSampleData\n",
        "      self.ProjectDir=\"/content/drive/MyDrive/UIowa/ISE/ISE6380/ChernobylBlueChillers/GANdinsky/\"\n",
        "      if True == self.UseSampleData:\n",
        "        # If using sample data, overwrite the parameters\n",
        "        project_dir=\"/content/ChernobylBlueChillers/\"\n",
        "      self.SampleUrl=\"https://drive.google.com/uc?export=download&id=1ZPY_-CFnrCiDucbM0kmQB0G6rgOjoXBs\"\n",
        "      self.SampleDir=self.ProjectDir\n",
        "      self.SampleBundle=self.SampleDir + 'SampleData.zip'\n",
        "\n",
        "      #photo directory information\n",
        "      self.ProjectLandscapePhotoDir=self.ProjectDir + \"landscape/photos/\"\n",
        "      self.ProjectLandscapeDir=self.ProjectDir + \"landscape/\"\n",
        "      #https://www.kaggle.com/arnaud58/landscape-pictures\n",
        "      self.PhotoBundleUrl=\"https://storage.googleapis.com/kaggle-data-sets/298806/1217826/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210228%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210228T202950Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=16593e15698fc6080632d46623d25f3a5e2181fe3d6a94eb70f27dba657ded3b4d1b73c850b64a96fa294d7ca2404794577386743f72f452c50d49073411729bcf16404c1695fffb9a6e9aff075cfa54906ac9b96352d4fb28f546a1f57a52b97b205541aaa7f60325e4d9a7e7054ec0d099b760cab8110f6517ec401c9c810bbee66a4bc2566e745da43d3c7d4957e10301d72bd086169789a0c184d90f1e5f68b96d8c16707c125ee5e83035a016bdf736b7a347384e88392395615d5cadd1274c535e956cdf00e27c4d78d07160b861886760f5d84e2e689470dd761976788671f6b08caf86a15fb3f87c79f39a66bf6eba6a02b8150daabe1297d6e5a2fc\"\n",
        "      self.PhotoBundle=self.ProjectLandscapePhotoDir + 'kaggle-landscape_photos.zip'\n",
        "\n",
        "      self.ProjectDnaDir=self.ProjectDir + \"DNAData/\"\n",
        "      #Bsubtilis_JRC DNA data\n",
        "      self.BsubtilisJRCBundleUrl=\"http://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/BIOINFORMATICS/Bacillus-subtilis/VER1-0/Bsubtilis-JRC.tgz\"\n",
        "      self.BsubtilisJRCBundle = self.ProjectDnaDir + 'Bsubtilis-JRC.tgz'\n",
        "      self.ProjectDnaBsubtilisJRCDir=self.ProjectDnaDir + \"Bsubtilis-JRC/\"\n",
        "      self.ProjectDnaBsubtilisJRCData = [self.ProjectDnaBsubtilisJRCDir + \"Bsubtilis-JRC.fastq\"]\n",
        "\n",
        "      #Bsubtilis_LGL DNA data\n",
        "      self.BsubtilisLGLBundleUrl=\"http://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/BIOINFORMATICS/Bacillus-subtilis/VER1-0/Bsubtilis-LGL.tgz\"\n",
        "      self.BsubtilisLGLBundle = self.ProjectDnaDir + 'Bsubtilis-LGL.tgz'\n",
        "      self.ProjectDnaBsubtilisLGLDir=self.ProjectDnaDir + \"Bsubtilis-LGL/\"\n",
        "      self.ProjectDnaBsubtilisLGLData = []#[self.ProjectDnaBsubtilisLGLDir + \"Bsubtilis-LGL.fastq\"]\n",
        "\n",
        "      #Bsubtilis_LHL DNA data\n",
        "      self.BsubtilisLHLBundleUrl=\"http://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/BIOINFORMATICS/Bacillus-subtilis/VER1-0/Bsubtilis-LHL.tgz\"\n",
        "      self.BsubtilisLHLBundle = self.ProjectDnaDir + 'Bsubtilis-LHL.tgz'\n",
        "      self.ProjectDnaBsubtilisLHLDir=self.ProjectDnaDir + \"Bsubtilis-LHL/\"\n",
        "      self.ProjectDnaBsubtilisLHLData = [self.ProjectDnaBsubtilisLHLDir + \"Bsubtilis_S1_L001_R1_001.fastq\"]\n",
        "\n",
        "      if True == self.UseSampleData:\n",
        "        # If using sample data, overwrite the parameters\n",
        "        self.ProjectDnaBsubtilisJRCData = [self.ProjectDnaBsubtilisJRCDir + \"Bsubtilis-JRC-truncated.fastq\"]\n",
        "        self.ProjectDnaBsubtilisLHLData = [self.ProjectDnaBsubtilisLHLDir + \"Bsubtilis_S1_L001_R1_001-truncated.fastq\"]\n",
        "\n",
        "      self.DNAData=[]\n",
        "      self.DNAData.extend(self.ProjectDnaBsubtilisJRCData)\n",
        "      self.DNAData.extend(self.ProjectDnaBsubtilisLGLData)\n",
        "      self.DNAData.extend(self.ProjectDnaBsubtilisLHLData)\n",
        "      self.DNADataRecords={}\n",
        "      \n",
        "      self.Noise=tf.Variable(tf.zeros([256]), dtype=tf.float32) \n",
        "      #tf.Variable(tf.ones(shape=[None, 256]), dtype=tf.float32)\n",
        "\n",
        "      prep_drive(not self.UseSampleData, [self.ProjectDir, self.ProjectLandscapePhotoDir, self.ProjectDnaDir])\n",
        "      os.chdir(self.ProjectDir)\n",
        "\n",
        "      self.ExtractAndLoadData()\n",
        "      self.PrepPipeline()\n",
        "    \n",
        "    def ExtractAndLoadData(self):\n",
        "      if self.UseSampleData:\n",
        "        ########################################################################################\n",
        "        # 1. Download and Extract the sample Data\n",
        "        download_and_extract_file(self.SampleUrl, self.SampleBundle, self.SampleDir)\n",
        "      else:\n",
        "        ########################################################################################\n",
        "        # 1. Download and Extract the image library\n",
        "        download_and_extract_file(self.PhotoBundleUrl, self.PhotoBundle, self.ProjectLandscapePhotoDir)\n",
        "        ########################################################################################\n",
        "        # 2. Download and Extract the DNA library\n",
        "        download_and_extract_file(self.BsubtilisJRCBundleUrl, self.BsubtilisJRCBundle, self.ProjectDnaDir)\n",
        "        ##This is VERY LARGE, need to figure out if we really want data this large.\n",
        "        ##It caused me to run out of colab disk space.\n",
        "        #download_and_extract_file(self.BsubtilisLGLBundleUrl, self.BsubtilisLGLBundle, self.ProjectDnaDir)\n",
        "        download_and_extract_file(self.BsubtilisLHLBundleUrl, self.BsubtilisLHLBundle, self.ProjectDnaDir)\n",
        "\n",
        "    def DisplaySamples(self):\n",
        "      class_names = self.train_ds.class_names\n",
        "      plt.figure(figsize=(4, 4))\n",
        "      for images, labels in self.train_ds.take(1):\n",
        "        for i in range(4):\n",
        "          ax = plt.subplot(2, 2, i + 1)\n",
        "          plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "          plt.title(class_names[labels[i]])\n",
        "          plt.axis(\"off\")\n",
        "\n",
        "    def display_image(fielname):\n",
        "      return PIL.Image.open(filename)\n",
        "\n",
        "    def PrepTrainingData(self):\n",
        "      landscape_data_dir = pathlib.Path(self.ProjectLandscapeDir)\n",
        "      image_count = len(list(landscape_data_dir.glob('*/*.jpg')))\n",
        "      #print(image_count)\n",
        "      landscape_photos = list(landscape_data_dir.glob('photos/*.jpg'))\n",
        "      #PIL.Image.open(str(landscape_photos[0]))\n",
        "\n",
        "      self.TrainingParam[\"batch_size\"]= 32\n",
        "      if (image_count < (self.TrainingParam[\"batch_size\"]*5)):\n",
        "        self.TrainingParam[\"batch_size\"] = int(image_count/5)\n",
        "\n",
        "      self.train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        landscape_data_dir,\n",
        "        validation_split=self.TrainingParam[\"validation_split\"],\n",
        "        subset=\"training\",\n",
        "        color_mode='rgb',\n",
        "        shuffle=True,\n",
        "        seed=self.TrainingParam[\"seed\"],\n",
        "        image_size=(self.TrainingParam[\"height\"], self.TrainingParam[\"width\"]),\n",
        "        batch_size=self.TrainingParam[\"batch_size\"])\n",
        "      class_names = self.train_ds.class_names\n",
        "      #print(class_names)\n",
        "\n",
        "      self.test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        landscape_data_dir,\n",
        "        validation_split=self.TrainingParam[\"validation_split\"],\n",
        "        subset=\"validation\",\n",
        "        seed=self.TrainingParam[\"seed\"],\n",
        "        image_size=(self.TrainingParam[\"height\"], self.TrainingParam[\"width\"]),\n",
        "        batch_size=self.TrainingParam[\"batch_size\"])\n",
        "      \n",
        "      normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "\n",
        "      # normalize the already shuffled training data\n",
        "      self.normalized_ds = self.train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "      #Don't flatten, so that a convolution layer can be run\n",
        "      #flatten_layer = tf.keras.layers.Flatten()\n",
        "      self.image_batch, self.labels_batch = next(iter(self.normalized_ds))\n",
        "      #first_image = image_batch[0]\n",
        "      #print(np.min(first_image), np.max(first_image))\n",
        "      \n",
        "\n",
        "    # function to be applied to each element in a dataset\n",
        "    def convert_to_gray(image, label):    # note each element is comprised of an image and a label\n",
        "      return tf.reduce_mean(image, axis=-1), label\n",
        "\n",
        "    def PrepNoiseData(self, shuffle=False):\n",
        "      for dataFile in self.DNAData:\n",
        "        print(dataFile)\n",
        "        self.DNADataRecords[dataFile]=[] \n",
        "        with open(dataFile, \"r\") as handle:\n",
        "          Z = np.zeros((256),dtype='float32')\n",
        "          recordCount=0\n",
        "          for record in SeqIO.parse(handle, \"fastq\"):\n",
        "            self.DNADataRecords[dataFile].append(record)\n",
        "            dl=[(0,(1,(2,3)[char!='g'])[char!='c'])[char!='a'] for char in record.lower()]\n",
        "            average=(sum(dl) / len(dl))/3\n",
        "            Z[recordCount]=average\n",
        "            recordCount+=1\n",
        "            if recordCount >= len(Z):\n",
        "              break\n",
        "          Z=np.interp(Z, (Z.min(), Z.max()), (0.0, 1.0))\n",
        "          if True == shuffle:\n",
        "            rng = np.random.default_rng()\n",
        "            rng.shuffle(Z)\n",
        "          self.Noise = tf.convert_to_tensor(Z, dtype=tf.float32)\n",
        "        \n",
        "    def PrepPipeline(self):\n",
        "      self.PrepTrainingData()\n",
        "      self.PrepNoiseData()\n",
        "\n",
        "    def compile(self):\n",
        "      super(GAN, self).compile()\n",
        "      self.d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "      self.g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "      self.loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "\n",
        "    def go(self):\n",
        "      gan.compile()\n",
        "      # To limit the execution time, we only train on 100 batches. You can train on\n",
        "      # the entire dataset. You will need about 20 epochs to get nice results.\n",
        "      gan.fit(normalized_ds.take(100) , epochs=1)\n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wXyBO1txUqr",
        "outputId": "ca5f9e5d-15ce-491c-b047-07322e2b7f53"
      },
      "source": [
        "gan = GANDinsky(useSampleData=True)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 60 files belonging to 1 classes.\n",
            "Using 48 files for training.\n",
            "Found 60 files belonging to 1 classes.\n",
            "Using 12 files for validation.\n",
            "/content/drive/MyDrive/UIowa/ISE/ISE6380/ChernobylBlueChillers/GANdinsky/DNAData/Bsubtilis-JRC/Bsubtilis-JRC-truncated.fastq\n",
            "/content/drive/MyDrive/UIowa/ISE/ISE6380/ChernobylBlueChillers/GANdinsky/DNAData/Bsubtilis-LHL/Bsubtilis_S1_L001_R1_001-truncated.fastq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG-8HguvIqAp",
        "outputId": "e835815e-9ee0-493a-c559-bf6fa890c84f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#print(gan.train_ds)\n",
        "#gan.go()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/UIowa/ISE/ISE6380/ChernobylBlueChillers/GANdinsky/DNAData/Bsubtilis-JRC/Bsubtilis-JRC-truncated.fastq\n",
            "/content/drive/MyDrive/UIowa/ISE/ISE6380/ChernobylBlueChillers/GANdinsky/DNAData/Bsubtilis-LHL/Bsubtilis_S1_L001_R1_001-truncated.fastq\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}